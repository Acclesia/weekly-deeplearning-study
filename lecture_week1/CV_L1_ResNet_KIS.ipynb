{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dedicated-silly",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "* 논문 링크: [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-platinum",
   "metadata": {},
   "source": [
    "## 0. 논문의 구조\n",
    "\n",
    "* Abstract와 1.Introduction에서 문제의식과 이를 해결하는 솔루션의 전체적 개요를 요약해서 제시한 후\n",
    "* 2.Related Work에서는 본 논문의 솔루션과 유사한 다른 시도들을 언급한다.\n",
    "* 3.Deep Residual Learning에는 제안 방법의 구체적인 내용과 구현 방법을 소개하고\n",
    "* 4.Experiments에서 비교실험 구조와 다양한 데이터셋에서의 실험결과를 통해 이를 검증한다.\n",
    "* 끝으로 References와 Appendix가 뒤를 잇는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-journal",
   "metadata": {},
   "source": [
    "## 1. ResNet 이전 모델들의 문제\n",
    "\n",
    "많은 모델들이 네트워크를 \"아주 깊게\" 쌓으면 기존보다 훨씬 좋은 성능이 나온다는 것을 여러 대회(ImageNet Challenge)를 통해 보여주었다.\n",
    "\n",
    "하지만 실제로는 Depth가 어느 부분까지 상승하다가 그 이상은 vanishing/exploding gradient라는 문제 때문에 오히려 성능이 떨어지는 현상이 나타났다. 이 현상을 `degradation problem`이라고 하는데, depth가 깊은 상태에서 학습을 이미 많이 진행한 경우 weight들의 분포가 균등하지 않고, 역전파시 기울기가 충분하지 않아 학습을 안정적으로 진행할 수 없게 되는 문제이다. over-fitting 과는 또 다른 문제이다.\n",
    "\n",
    "(Vanishing/Exploding Gradient 문제의 해결책에는 normalized initialization,  intermediate normalization layers가 있다.)\n",
    "\n",
    "<img src=\"./image/resnet.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-produce",
   "metadata": {},
   "source": [
    "## 2. Residual Block\n",
    "\n",
    "<img src=\"./image/residual_block.png\" />\n",
    "\n",
    "레이어를 많이 쌓았다고 해서 모델 성능이 떨어지는 것을 해결하기 위해서 `Residual Block`이라는 개념이 나온다. \n",
    "\n",
    "<img src=\"./image/residual_block2.png\" />\n",
    "\n",
    "\n",
    "* Residual 레이어를 $F(x)$로 표현하면 이 레이어의 결과는 입력값 $(x)$에 대해 $(F(x))$가 된다. \n",
    "* 여기에 레이어의 입력값 $(x)$을 더해주면 최종 출력값은 $(F(x)+x)$, 즉 레이어의 결과값이 된다. \n",
    "* 이후 이 값은 ReLU 활성함수(activation function)을 거치게 된다. \n",
    "* 위 식에서 $(F(x,W_{i}))$ 는 학습되어야 할 residual mapping으로서 잔차 학습(residual learning)은 이 식을 학습한다. \n",
    "* ResNet에서는 shortcut connection을 가진 ResNet의 기본 블록을 Residual Block이라고 부른다. ResNet은 이러한 Residual Block 여러 개로 이루어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-boost",
   "metadata": {},
   "source": [
    "### 논리\n",
    "\n",
    "$F(x)$가 Vanishing Gradient현상으로 전혀 학습이 안되어 zero mapping이 될지라도, 최종 $H(x)$는 최소한 identity mapping이라도 되니 성능 저하는 발생하지 않게 된다.<br> \n",
    "이렇게 하면 실제로 학습해야 할 $F(x)$는 학습해야 할 레이어 $H(x)$에다 입력값 $(x)$를 뺀 형태, 즉 잔차(Residual)함수가 되는데, 이것은 $H(x)$를 직접 학습하는 것보다는 훨씬 학습이 쉬워진다.<br> \n",
    "레이어를 깊이 쌓을 수록 Residual에 해당하는 $(F(x))$는 0에 가까운 작은 값으로 수렴해도 충분하기 때문이다. 그리고 실험해 보니 이 구조가 실제로도 안정적으로 학습이 되며, 레이어를 깊이 쌓을수록 성능이 향상되는 것이 확인되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-infrastructure",
   "metadata": {},
   "source": [
    "## 3. Experiments\n",
    "\n",
    "<img src=\"./image/resnet_error.png\" />\n",
    "\n",
    "* 18개 층과 34개 층을 갖는 네트워크를, 각각 shortcut이 없는 일반 네트워크(plain network)와 shortcut이 있는 ResNet 두 가지로 구현해 총 4가지를 비교\n",
    "\n",
    "<img src=\"./image/resnet_error2.png\" />\n",
    "\n",
    "`Table 2.`는 이미지넷 검증 데이터셋을 사용해 실험한 결과를 나타낸다.\n",
    "\n",
    "일반 네트워크(\"plain\")는 레이어가 16개나 늘어나 네트워크가 깊어졌는데도 오류율은 오히려 높아졌다. 경사소실로 인해 훈련이 잘 되지 않았기 때문이다. ResNet에서는 잘 훈련된 레이어가 16개가 늘어난 효과로 오류율이 2.85% 감소했다. 논문에서는 이렇게 간단한 실험으로 Residual Block의 효과를 입증하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-polls",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
