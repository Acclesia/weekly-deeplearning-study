{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electronic-underground",
   "metadata": {},
   "source": [
    "# AIFFEL 대전 1기 Going Deeper CV\n",
    "## Date: APRIL 05, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-nudist",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "- `Deep Residual Learning for Image Recognition`이라는 논문에서 나온 딥러닝 모델  \n",
    "- `Residual Block`이라는 아주 간단하면서도 획기적인 개념을 도입하여 딥러닝 모델의 레이어가 깊어져도 안정적으로 학습되면서 모델 성능 개선까지 가능함을 입증함  \n",
    "\n",
    "## 1. 왜 ResNet이 등장하게 되었나?\n",
    "- `Degradation Problem`을 해결하기 위해  \n",
    "Degradation Problem이란? 딥러닝 모델의 레이어를 깊이 쌓았을 때 모델이 수렴하고 있음에도 불구하고 발생하는 문제. **레이어 개수가 더 적을 때보다 모델의 train/test error이 커지는 현상.** 이는 오버피팅때문이 아니라 네트워크 구조상 레이어를 깊이 쌓았을 때 **최적화가 잘 안되기 때문에 발생**하는 문제  \n",
    "\n",
    "\n",
    "(참고) 레이어를 깊게 쌓다보면 기울기 Vanshing/Exploding 문제가 발생할 수 있다. 위의 언급한 Degradation Problem과는 다름.  \n",
    "Vanshing/Exploding 문제해결 위한 방안: normalized initialization,  intermediate normalization layers 등  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-specific",
   "metadata": {},
   "source": [
    "## 2. ResNet이 제시한 솔루션: Residual Block\n",
    "- 기본 아이디어: 깊은 네트워크의 학습이 어려운 점을 해결하기 위해서 **레이어의 입력값을 활용**하여 **레이어가 \"residual function\"(잔차 함수)을 학습하도록** 한다.  \n",
    "쉬운 표현으로 **일종의 지름길(\"shortcut connection\")을 통해서 레이어가 입력값을 직접 참조하도록 레이어를 변경**  \n",
    "- `shortcut connection`은 입력값을 네트워크 출력층에 곧바로 더해준다.  \n",
    "- 이렇게 되면 네트워크는 **출력값에서 원본 입력을 제외한 잔차(residual)함수를 학습**하게 된다. 여기서 네트워크 이름인 **ResNet**이 나오게 되었다.  \n",
    "![resnet](https://aiffelstaticprd.blob.core.windows.net/media/original_images/GC-1-L-2.png)  \n",
    "\n",
    "- ResNet에서는 short connection을 가진 ResNet의 기본 블록을 Residual Block이라고 부른다. ResNet은 이러한 Residual Block 여러 개로 이루어져 있다.  \n",
    "\n",
    "\n",
    "- 수식으로 알아보기  \n",
    "1) 학습해야 할 레이어 $(H(x))$를 $(F(x) + x)$로 만들기  \n",
    "2) 만약 기울기 Vanshing이 일어나 전혀 학습이 되지 않아 zero mapping이 되어도, **최종 $H(x)$는 최소한 identity mapping이라도 되니 성능 저하가 발생하기 않는다.**  \n",
    "3) 즉 $F(x) = H(x) - (x)$이 되게 되며, 이는 잔차 함수(residual function)가 되며, 이는 $H(x)$를 직접 학습하는 것보다는 훨씬 학습이 쉽다.  \n",
    "이유: 레이어를 깊이 쌓을 수록 **Residual에 해당하는 F(x)**는 0에 가까운 작은 값으로 수렴해도 괜찮기 때문. (뒤에 +x가 있으니까)  \n",
    "\\> 실제로 실험을 통해 안정적인 학습을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-trading",
   "metadata": {},
   "source": [
    "## 3. Experiments\n",
    "- 18개 층과 34개 층을 갖는 네트워크를, 각각 shortcut이 없는 일반 네트워크와 shortcut이 있는 ResNet 두 가지로 구현해 비교  \n",
    "\n",
    "![ResNet Experiments](https://aiffelstaticprd.blob.core.windows.net/media/images/GC-1-L-5.max-800x600.png)  \n",
    "- 왼쪽 그림은 일반 네트워크 두 개로 네트워크가 깊어지더라도 학습이 잘 되지 않는다.  \n",
    "- 오른쪽 그림은 shortcut이 적용된 그래프를 보면 레이어가 깊어져도 학습이 잘되는 효과를 볼 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-thousand",
   "metadata": {},
   "source": [
    "(참고) SOTA(State of the art: 벤치마크 데이터셋을 이용해 고성능을 발휘하는 모델의 성능  \n",
    "(참고) 벤치마크 데이터셋: 어떤 task의 모델 간 성능을 비교할 때 널리 사용되어 기준이 되는 데이터셋"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
